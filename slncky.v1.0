#!/usr/bin/env python

import os
import sys
import argparse
import subprocess
import tempfile
from multiprocessing import Lock, Process, Queue
import time
from copy import deepcopy
import math


REALPATH = ''
ALIGNTRANSCRIPTS = ''
CONFIG = ''
LASTZ = 'lastz'
BEDTOOLS = 'bedtools'
FASTAFROMBED = 'fastaFromBed'
INTERSECTBED = 'intersectBed'
LIFTOVER = 'liftOver'

def checkDependencies():
	#check bedtools version
	cmd = [BEDTOOLS, '--version']
	try:
		out = subprocess.check_output(cmd)
	except:
		sys.exit("ERROR: bedtools not installed! You must have bedtools v2.17.0 or higher in your path! Exiting...")

	out = out.split()
	version = out[1][1:].split(".")
	if int(version[0]) > 2 or int(version[1]) < 17:
		sys.exit("ERROR: The version of bedtools you have installed is too old. You must have bedtools v2.17.0 or higher in your path! Exiting...")

	#check lastz
	cmd = [LASTZ, '--version']
	try:
		p = subprocess.Popen(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
	except:
		sys.exit("ERROR: lastz not installed! You must have lastz in your path! Exiting...")

	out = p.stdout.read()
	out = out.split()
	if out[0].strip() != "lastz":
		sys.exit("ERROR: lastz not installed! You must have lastz in your path! Exiting...")

	#check lastz
	cmd = [LIFTOVER]
	try:
		p = subprocess.Popen(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
	except:
		sys.exit("ERROR: liftOver not installed! You must have liftOver in your path! Exiting...")

	out = p.stderr.read()
	out = out.split()
	if out[0].strip() != "liftOver":
		sys.exit("ERROR: liftOver not installed! You must have liftOver in your path! Exiting...")

def readAnnots(config, assembly):
	ANNOTS = {}

	##READ IN CONFIG FILE##
	try:
		config = open(config, 'r')
	except IOError:
		print "ERROR: cannot open ", config
	
	flag = False
	for line in config.readlines():
		line = line.strip()
		if line == "" or line[0]=="#": continue
		
		if line[0] == ">" and line[1:] == assembly.strip():
			flag=True
		if line[0] == ">" and line[1:] != assembly.strip():
			flag=False

		if flag and line[0] != ">":
			line = line.split("=")
			file = line[1].strip()
			if line[0] != "ORTHOLOG": 
				if not os.path.isfile(file): 
					ogFile = file
					file = REALPATH+file
				if not os.path.isfile(file):
					print line[0]+" file "+ogFile+" does not exist! Please check your annotations.config file.  Exiting..."
					sys.exit(1)

			if line[0].strip() in ANNOTS:
				arr = ANNOTS[line[0].strip()]
				arr.append(file)
				ANNOTS[line[0].strip()] = arr
			else:
				ANNOTS[line[0].strip()] = [file]
	
	#CHECK ANNOTS
	if len(ANNOTS) == 0:
		print "WARNING: no annotations found for %s" % assembly
	else:
		if 'CODING' not in ANNOTS:
			print "WARNING: no coding file was supplied in annotations.config for %s. Cannot find overlap with coding genes." % assembly
		if 'GENOME_FA' not in ANNOTS:
			print "WARNING: no genome fasta file was supplied in annotations.config for %s.  Cannot blast to find duplications or matches to orthologous coding genes." %assembly

	return ANNOTS

def writeToTmp(dict):
	tempFd, tempPath = tempfile.mkstemp()

	temp = open(tempPath, 'w')
	for key,entry in dict.iteritems():
		temp.write(entry)
	temp.close()
	os.close(tempFd)

	return tempPath

def removeLncs(lncs, remove):
	counter = 0
	for item in remove:
		if item[0] in lncs: 
			del lncs[item[0]]
			counter += 1
	return lncs, counter

def splitToExons(lncs):
	
	lncExonFd, lncExonPath = tempfile.mkstemp()
	lncExon = os.fdopen(lncExonFd, 'w')

	lncSize = {}

	for lnc, line in lncs.iteritems():
		line = line.split()
		chr = line[0]
		start = int(line[1])
		name = line[3]
		score = 0
		strand = line[5]
		numBlocks = int(line[9].strip())
		blockSizes = line[10].split(",")
		blockStarts = line[11].split(",")
		
		size = 0
		for i in range(numBlocks):
			size += int(blockSizes[i])

			exonStart = start + int(blockStarts[i])
			exonEnd = exonStart + int(blockSizes[i])
			lncExon.write("%s\t%d\t%d\t%s\t%s\t%s\n" % (chr,exonStart,exonEnd,name,score, strand))
		lncSize[lnc] = size

	lncExon.close()

	return lncExonPath, lncSize

def removeOverlap(lncs, annots, args, min):
	FILTER = []

	lncExonPath, lncSize = splitToExons(lncs)

	cmd = INTERSECTBED
	for arg in args:
		cmd += " "+arg
	for file in annots:
		pairToOverlap = {}
		cmd += " -wao -a %s -b %s" % (lncExonPath, file)
		out = subprocess.check_output([cmd], shell=True)
		out = out.split("\n")	
		for line in out:
			if line.strip() == "": continue
			line= line.split()
			lnc = line[3].strip()
			gene = line[9].strip()
			if gene == ".": continue
			pair = lnc+"&"+gene
			overlap = int(line[18])
			if pair in pairToOverlap:
				newOverlap = pairToOverlap[pair] + overlap
				pairToOverlap[pair] = newOverlap
			else:
				pairToOverlap[pair] = overlap
		for pair, overlap in pairToOverlap.iteritems():
			pair = pair.split("&")
			lnc = pair[0]
			gene = pair[1]
			overlap = overlap*1.0 / lncSize[lnc]
			if overlap > min:
				FILTER.append([lnc, gene, overlap, file])
	
	os.remove(lncExonPath)	
	FILTER.sort(key=lambda x: x[2], reverse=True)

	newLncs, numRemoved = removeLncs(lncs, FILTER)
	
	return newLncs, FILTER, numRemoved

def alignTranscripts(lncPath, fa, orthPath, orthFa, tmpPath, FILTER, min, null):
	

	if BEDTOOLS[0:-9] != "" and LASTZ[0:-6] != "":
		cmd = [ALIGNTRANSCRIPTS, lncPath, fa, orthPath, orthFa, tmpPath, '--bedtools_path', BEDTOOLS[0:-8], '--lastz_path', LASTZ[0:-5]]
	elif BEDTOOLS[0:-8] != "":
		cmd = [ALIGNTRANSCRIPTS, lncPath, fa, orthPath, orthFa, tmpPath, '--bedtools_path', BEDTOOLS[0:-8]]
	elif LASTZ[0:-5] != "":
		cmd = [ALIGNTRANSCRIPTS, lncPath, fa, orthPath, orthFa, tmpPath, '--lastz_path', LASTZ[0:-5]]
	else:
		cmd = [ALIGNTRANSCRIPTS, lncPath, fa, orthPath, orthFa, tmpPath]
	
	ret = subprocess.call(cmd, stdout = null, stderr = null)

	if ret == 0:

		outFile = open(tmpPath+".alignment_identity.txt", 'r')
		out = outFile.readlines()
		outFile.close()

		exonID = 0
		seqID = 0
		for line in out:
			if line.strip() == "": continue
			line = line.split()
	
			if line[3] != "exonID" and float(line[3]) > min and float(line[3]) > exonID:
				exonID = float(line[3])
				seqID = float(line[4])
		if exonID > 0:		
			FILTER.put([line[1], line[2], exonID, seqID])

		os.remove(tmpPath+".alignment_identity.txt")
		os.remove(tmpPath+".maf")

	FILTER.put(0)

	os.remove(lncPath)
	os.remove(orthPath)
	

def blastToOrtholog(lncs, annots, orth_annots, blastTo, min, n):
	queue = Queue()
	FILTER = []
	FLUSHED = 0
	lncFile = writeToTmp(lncs)
	
	liftOverFd, liftOverPath = tempfile.mkstemp()
	unmappedPath = liftOverPath+".unmapped"


	#liftover
	cmd = "cut -f1-4 %s | %s -minMatch=0.1 /dev/stdin %s %s %s" % (lncFile, LIFTOVER, annots["LIFTOVER"][0], liftOverPath, unmappedPath)
	null = open("/dev/null", 'w')
	proc = subprocess.check_call([cmd],shell=True, stdout=null, stderr=null)
	null.close()
	os.remove(lncFile)

	removed = 0
	processes = []
	null = open("/dev/null", 'w')
	for file in orth_annots[blastTo]:
		numAligned = 0
		cmd = [INTERSECTBED, '-wa', '-wb', '-a', liftOverPath, '-b', file]
		#print cmd
		out = subprocess.check_output(cmd)
		out = out.split("\n")
		for line in out:
			numAligned += 1
			if line.strip() == "": continue
			line= line.split()
			lnc = line[3].strip()
			lncBed = lncs[lnc]
			
			overlapBed = line[4]
			for i in range(5, len(line)):
				overlapBed += "\t"+line[i]
	
			lncFd, lncPath = tempfile.mkstemp()
			lncFile = os.fdopen(lncFd, 'w')
			lncFile.write(lncBed)
			lncFile.close()

			orthPath = lncPath+".orth"
			orthFile = open(orthPath, 'w')
			orthFile.write(overlapBed)
			orthFile.close()

			tmpOutPath = orthPath+".out"
			
			p = Process(target=alignTranscripts, args=(lncPath, annots["GENOME_FA"][0], orthPath, orth_annots["GENOME_FA"][0], tmpOutPath, queue, min, null))
			p.start()
			#pids[p.pid] = [lncFile, file, gene, overlap]

			processes.append(p)
			print "\r    aligning %d / %d genes to ortholog in %s" % (numAligned, len(out), file),
			sys.stdout.flush()
			while True:
				counter = 0
				
				for p in processes:
					if p.is_alive(): 
						counter += 1
				if counter < n: break
				else: 
					queue.put('STOP')
					for item in iter(queue.get, 'STOP'):
						if item == 0: FLUSHED += 1
						else: FILTER.append(item)
					time.sleep(1)
	null.close()	
	while FLUSHED < len(processes):
		queue.put('STOP')
		for item in iter(queue.get, 'STOP'):
			if item == 0: FLUSHED += 1
			else: FILTER.append(item)


	for p in processes:
		p.join()

	os.remove(liftOverPath)
	os.remove(unmappedPath)

	#newLncs, numRemoved = removeLncs(lncs, FILTER)

	return FILTER

def readMaf(maf, FILTER, cov_min, id_min):
	selfBlast = {}
	
	for i in range(16, len(maf)):
		line = maf[i]
		if line == "": continue
		if i % 8 == 0:
			identity = line.split(" ")[2].strip()[1:-2]
		if i % 8 == 1:
			coverage = line.split(" ")[2].strip()[1:-2]
		if i % 8 == 2:
			continuity = line.split(" ")[2].strip()[1:-2]
		if i % 8 == 5:
			name = line.split(" ")[1].strip()
		if i % 8 == 6:
			queryName = line.split(" ")[1].strip()
			if name.strip() != queryName.strip() and float(identity) >= cov_min*100 and float(coverage) >= id_min*100:
				pair = name.strip()+"&"+queryName.strip()
				if pair in selfBlast:
					if float(identity) > selfBlast[pair][0]:
						selfBlast[pair] = [float(identity), float(coverage)]
				else:
					selfBlast[pair] = [float(identity), float(coverage)]

	for pair, id in selfBlast.iteritems():
		pair = pair.split("&")
		FILTER.put([pair[0], pair[1], id[0], id[1]])


	del selfBlast

def selfBlastOneLnc(curLnc, lncs, FILTER, cov_min, id_min, tmpPath):
	#cmd = "%s %s %s --format=maf+" % (LASTZ, curLnc, lncs)
	#cmd = [LASTZ, curLnc, lncs, '--format=maf+']
	cmd = [LASTZ, curLnc, lncs, '--format=maf+', '--output=%s' % tmpPath]
	subprocess.call(cmd)

	file = open(tmpPath, 'r')
	out = file.readlines()
	file.close()
	readMaf(out, FILTER, cov_min, id_min)

	#cmd = ['rm', '-f', curLnc, lncs, tmpPath]
	#subprocess.check_call(cmd)
	os.remove(curLnc)
	os.remove(lncs)
	os.remove(tmpPath)

	FILTER.put(0)

def selfBlast(lncs, annots, cov_min, id_min, n):
	queue = Queue()

	#for every lnc
		#add blastOneLnc process
	processes = []
	numAligned = 0
	lncFile = writeToTmp(lncs)
	
	FILTER = []
	FLUSHED = 0
	
	for lnc, entry in lncs.iteritems():
		
		#make target fasta
		curLnc = {}
		curLnc[lnc] = entry
		curLncFile = writeToTmp(curLnc)
		curLncFaPath = curLncFile+".fa"

		cmd = [FASTAFROMBED, '-fi', annots["GENOME_FA"][0], '-bed', curLncFile, '-fo', curLncFaPath, '-split', '-name']
		subprocess.check_call(cmd)
		
		#make query fasta
		queryLncBedPath = curLncFile+".query.bed"
		#cmd = "%s -v -a %s -b %s > %s" % (INTERSECTBED, lncFile, curLncFile, queryLncBedPath)
		cmd = [INTERSECTBED, '-v', '-a', lncFile, '-b', curLncFile] 
		out = subprocess.check_output(cmd)
		queryLncBed = open(queryLncBedPath, 'w')
		queryLncBed.write(out)
		queryLncBed.close()

		queryLncFaPath = curLncFile+".query.fa"
		cmd = [FASTAFROMBED, '-fi', annots["GENOME_FA"][0], '-bed', queryLncBedPath, '-fo', queryLncFaPath, '-split', '-name']
		subprocess.call(cmd)
		
		mafBedPath = curLncFile+".maf"
		
		os.remove(queryLncBedPath)
		os.remove(curLncFile)
		
		p = Process(target=selfBlastOneLnc, args=(curLncFaPath, queryLncFaPath, queue, cov_min, id_min, mafBedPath))
		numAligned += 1
		p.start()
		processes.append(p)
		
		print "\r    self-blasting %d / %d genes" % (numAligned, len(lncs)),
		sys.stdout.flush()
		while True:
			counter = 0
			for p in processes:
				if p.is_alive(): counter += 1
			if counter < n*4: break
			else: 
				queue.put('STOP')
				for item in iter(queue.get, 'STOP'):
					if item == 0: FLUSHED += 1
					else: FILTER.append(item)
				time.sleep(1)
	

	while (FLUSHED < len(processes)):
		queue.put('STOP')
		for item in iter(queue.get, 'STOP'):
			if item == 0: FLUSHED += 1
			else: FILTER.append(item)

	for p in processes:
		p.join()

	newLncs, numRemoved = removeLncs(lncs, FILTER)
	
	return newLncs, FILTER, numRemoved
	#return lncs, FILTER, 0

def repeatFilter(lncs, annots, repeat_min):
	lncExonPath, lncSize = splitToExons(lncs)

	cmd = "%s -wo -a %s -b %s" % (INTERSECTBED, lncExonPath, annots["REPEAT_MASKER"][0])
	out = subprocess.check_output([cmd], shell=True)
	out = out.split("\n")
	
	lncOverlap = {}
	for line in out:
		if line.strip() == "": continue
		line = line.split()
		lnc = line[3].strip()
		overlap = int(line[18])
		if lnc in lncOverlap:
			a = lncOverlap[lnc]
			lncOverlap[lnc] = a + overlap
		else: lncOverlap[lnc] = overlap
	
	os.remove(lncExonPath)
	FILTER = []
	REPEAT = []

	for lnc, overlap in lncOverlap.iteritems():
		percent = overlap*1.0 / lncSize[lnc]
		if (percent > repeat_min):
			FILTER.append([lnc, percent])
		REPEAT.append([lnc, percent])
	#newLncs, numRemoved = removeLncs(lncs, FILTER)
	
	return lncs, FILTER, 0, REPEAT


IFRAME = '''
	<script type="text/javascript">
	<!--//
	function goToLnc(mySelect) {
	PageIndex2=mySelect.selectedIndex;
	{
	if (mySelect.options[PageIndex2].value != "none") {
		//this is the key code in the JavaScript to open the new page in the iframe:-
		frames['iframe'].location.href = mySelect.options[PageIndex2].value;
	}
	}
	}

	//-->
	</script>
'''

def writeOverlappingTranscripts(out, ogLncs, SENSE_FILTER, ANTISENSE_FILTER, SENSE_MAPPED_CODING_FILTER, ANTISENSE_MAPPED_CODING_FILTER):
	global IFRAME
	
	pre = out+"_html/"

	html = open(pre+"overlapCoding.html", 'w')
	html.write("<html><head>%s</head><body>\n" % IFRAME)
	

	html.write("<h2>Overlapping Transcripts</h2>\n")
	html.write("<p>Step 1: Load %s.OVERLAP.bed to the UCSC session below by clicking My Data -> Custom Tracks.\n" % out)
	html.write("<p>Step 2: Select transcript from drop-down menu to view: \n")
	html.write("<form name=\"overlapping\">\n")
	html.write("<select name=\"selectLnc\" onChange=\"goToLnc(this.form.selectLnc)\">\n")

	html.write("<option value=\"None\" style=\"font-weight: bold;\">>>OVERLAPS CODING</option>\n")
	for x in SENSE_FILTER:
		lncEntry = ogLncs[x[0]].split("\t")
		chr = lncEntry[0]
		start = lncEntry[1]
		end = lncEntry[2]
		html.write("<option value=\"http://genome.ucsc.edu/cgi-bin/hgTracks?db=mm9&position=%s:%s-%s\">%s - %.1f%% sense</option>\n" % (chr, start, end, x[0], x[2]*100.0))
	for x in ANTISENSE_FILTER:
		lncEntry = ogLncs[x[0]].split("\t")
		chr = lncEntry[0]
		start = lncEntry[1]
		end = lncEntry[2]
		html.write("<option value=\"http://genome.ucsc.edu/cgi-bin/hgTracks?db=mm9&position=%s:%s-%s\">%s - %.1f%% antisense</option>\n" % (chr, start, end, x[0], x[2]*100.0))
	
	html.write("<option value=\"None\" style=\"font-weight: bold;\">>>OVERLAPS MAPPED CODING</option>")
	for x in SENSE_MAPPED_CODING_FILTER:
		lncEntry = ogLncs[x[0]].split("\t")
		chr = lncEntry[0]
		start = lncEntry[1]
		end = lncEntry[2]
		html.write("<option value=\"http://genome.ucsc.edu/cgi-bin/hgTracks?db=mm9&position=%s:%s-%s\">%s - %.1f%% sense</option>\n" % (chr, start, end, x[0], x[2]*100.0))
	for x in ANTISENSE_MAPPED_CODING_FILTER:
		lncEntry = ogLncs[x[0]].split("\t")
		chr = lncEntry[0]
		start = lncEntry[1]
		end = lncEntry[2]
		html.write("<option value=\"http://genome.ucsc.edu/cgi-bin/hgTracks?db=mm9&position=%s:%s-%s\">%s - %.1f%% antisense</option>\n" % (chr, start, end, x[0], x[2]*100.0))
	
	html.write("</select>\n</form>\n")
	
	html.write("<iframe name=\"iframe\" src=\"http://genome.ucsc.edu/cgi-bin/hgTracks?hgS_doOtherUser=submit&hgS_otherUserName=jjennychen&hgS_otherUserSessionName=filterLncs.mm9\" width=100% height=\"1000\"></iframe>\n")
	html.write("</body></html>")

def writeCodingBlast(out, CODING_BLAST_FILTER):
	pre = out+"_html/"

	html = open(pre+"blastCoding.html", 'w')

	html.write("<html><head></head><body\n")
	html.write("<h2>Transcripts Blasting to Orthologous Coding Gene</h2>\n")
	html.write("This is a place holder for the visualization tool which will show the alignments.\n")
	html.write("</body></html>\n")

def makeClusters(SELF_BLAST_FILTER):
	#create clusters
	sets = []
	pairsToId = {}

	for x in SELF_BLAST_FILTER:
		pair = x[0]+"&"+x[1]
		id = float(x[3])
		if (pair in pairsToId and id > pairsToId[pair]) or (pair not in pairsToId):
			pairsToId[pair] = id
		
		newSet = set()
		newSet.add(x[0])
		newSet.add(x[1])
		sets.append(newSet)
	
	merged = True
	while merged:
		merged = False
		results = []
		while sets:
			common, rest = sets[0], sets[1:]
			sets = []
			for x in rest:
				if x.isdisjoint(common):
					sets.append(x)
				else:
					merged = True
					common |= x
			results.append(common)
		sets = results
	return sets, pairsToId

def writeJson(out, sets, pairsToId):
	pre = out+"_html/"
	lncToNum = {}
	#write json

	json = open(pre+out+".duplications.json", 'w')
	json.write("{\"nodes\": [")
	
	counter = -1
	for i in range(len(sets)):
		x = sets[i]
		for lnc in x:
			counter += 1
			lncToNum[lnc] = counter
			if counter == 0:
				json.write("{\"name\": \"%s\", \"group\": %d}" % (lnc, i+1))
			else:
				json.write(", {\"name\": \"%s\", \"group\": %d}" % (lnc, i+1))
	
	json.write("], ")

	json.write("\"links\":[")
	
	counter = 0
	for pair, id in pairsToId.iteritems():
		counter += 1
		pair = pair.split("&")
		source = lncToNum[pair[0]]
		target = lncToNum[pair[1]]
		if counter == 1: json.write("{\"source\": %d, \"target\": %d, \"value\": %d}" % (source, target, int(round(id))))
		else:  json.write(", {\"source\": %d, \"target\": %d, \"value\": %d}" % (source, target, int(round(id))))

	for lnc, num in lncToNum.iteritems():
		json.write(", {\"source\": %d, \"target\": %d, \"value\": 100}" % (num, num))
	json.write("]}")

HEATMAP_HEAD = '''
<style>
@import url(http://bost.ocks.org/mike/style.css?aea6f0a);

.background {
  fill: #eee;
}

line {
  stroke: #fff;
}

text.active {
  fill: red;
}

</style>

'''
HEATMAP_SCRIPT = '''

	<script src="http://d3js.org/d3.v2.min.js?2.8.1" charset="utf-8" type="text/javascript"></script>
	<script type="text/javascript">

	var margin = {top: 80, right: 0, bottom: 10, left: 80},
		width = 720,
		height = 720;

	var x = d3.scale.ordinal().rangeBands([0, width]),
		z = d3.scale.linear().domain([40, 100]).clamp(true),
		c = d3.scale.category10().domain(d3.range(10));

	var svg = d3.select("body").append("svg")
		.attr("width", width + margin.left + margin.right)
		.attr("height", height + margin.top + margin.bottom)
		.style("margin-left", margin.left + "px")
	  .append("g")
		.attr("transform", "translate(" + margin.left + "," + margin.top + ")");

	d3.json("REPLACEME", function(miserables) {
	  var matrix = [],
		  nodes = miserables.nodes,
		  n = nodes.length;

	  // Compute index per node.
	  nodes.forEach(function(node, i) {
		node.index = i;
		node.count = 0;
		matrix[i] = d3.range(n).map(function(j) { return {x: j, y: i, z: 0}; });
	  });

	  // Convert links to matrix; count character occurrences.
	  miserables.links.forEach(function(link) {
		matrix[link.source][link.target].z = link.value;
		matrix[link.target][link.source].z = link.value;
		//matrix[link.source][link.source].z += link.value;
		//matrix[link.target][link.target].z += link.value;
		nodes[link.source].count = link.value;
		nodes[link.target].count = link.value;
	  });

	  // Precompute the orders.
	  var orders = {
		name: d3.range(n).sort(function(a, b) { return d3.ascending(nodes[a].name, nodes[b].name); }),
		count: d3.range(n).sort(function(a, b) { return nodes[b].count - nodes[a].count; }),
		group: d3.range(n).sort(function(a, b) { return nodes[b].group - nodes[a].group; })
	  };

	  // The default sort order.
	  x.domain(orders.group);

	  svg.append("rect")
		  .attr("class", "background")
		  .attr("width", width)
		  .attr("height", height);

	  var row = svg.selectAll(".row")
		  .data(matrix)
		.enter().append("g")
		  .attr("class", "row")
		  .attr("transform", function(d, i) { return "translate(0," + x(i) + ")"; })
		  .each(row);

	  row.append("line")
		  .attr("x2", width);

	  row.append("text")
		  .attr("x", -6)
		  .attr("y", x.rangeBand() / 2)
		  .attr("dy", ".32em")
		  .attr("text-anchor", "end")
		  .text(function(d, i) { return nodes[i].name; });

	  var column = svg.selectAll(".column")
		  .data(matrix)
		.enter().append("g")
		  .attr("class", "column")
		  .attr("transform", function(d, i) { return "translate(" + x(i) + ")rotate(-90)"; });

	  column.append("line")
		  .attr("x1", -width);

	  column.append("text")
		  .attr("x", 6)
		  .attr("y", x.rangeBand() / 2)
		  .attr("dy", ".32em")
		  .attr("text-anchor", "start")
		  .text(function(d, i) { return nodes[i].name; });

	  function row(row) {
		var cell = d3.select(this).selectAll(".cell")
			.data(row.filter(function(d) { return d.z; }))
		  .enter().append("rect")
			.attr("class", "cell")
			.attr("x", function(d) { return x(d.x); })
			.attr("width", x.rangeBand())
			.attr("height", x.rangeBand())
			.style("fill-opacity", function(d) { return z(d.z); })
			.style("fill", "#D62728")
			.on("mouseover", mouseover)
			.on("mouseout", mouseout);
		
		var cell = d3.select(this).selectAll(".row")
			.data(row.filter(function(d) { return d.z;}))
			.enter().append("text")
			.attr("class", "cell")
		   .attr("x", function(d) { return x(d.x) + (x.rangeBand()/2); })
			.attr("y", function(d) { return x.rangeBand() / 2; })
			.text(function(d) {return d.z+"%";});
	   
	  }


	  function mouseover(p) {
		d3.selectAll(".row text").classed("active", function(d, i) { return i == p.y; });
		d3.selectAll(".column text").classed("active", function(d, i) { return i == p.x; });
	  }

	  function mouseout() {
		d3.selectAll("text").classed("active", false);
	  }

	  //d3.select("#order").on("change", function() {
	  //  clearTimeout(timeout);
	  //  order(this.value);
	  //});

	  function order(value) {
		x.domain(orders[value]);

		var t = svg.transition().duration(2500);

		t.selectAll(".row")
			.delay(function(d, i) { return x(i) * 4; })
			.attr("transform", function(d, i) { return "translate(0," + x(i) + ")"; })
		  .selectAll(".cell")
			.delay(function(d) { return x(d.x) * 4; })
			.attr("x", function(d) { return x(d.x); });

		t.selectAll(".column")
			.delay(function(d, i) { return x(i) * 4; })
			.attr("transform", function(d, i) { return "translate(" + x(i) + ")rotate(-90)"; });
	  }

	  //var timeout = setTimeout(function() {
	  //  order("group");
	  //  d3.select("#order").property("selectedIndex", 2).node().focus();
	  //}, 5000);
	});

	</script>
	'''
def writeDuplications(out, SELF_BLAST_FILTER, id_min):
	global HEATMAP_HEAD
	global HEATMAP_SCRIPT
	
	pre = out+"_html/"

	html = open(pre+"selfBlast.html", 'w')
	html.write("<html><head>%s</head><body>\n" % HEATMAP_HEAD)
	
	#genea, geneb, coverage, id, continuity
	sets, pairsToId = makeClusters(SELF_BLAST_FILTER)

	writeJson(out, sets, pairsToId)

	HEATMAP_SCRIPT = HEATMAP_SCRIPT.replace("REPLACEME", out+".duplications.json")

	#write website
	html.write("<h2>Duplications</h2>\n")
	html.write("The following transcripts aligned to another transcript with >%d%% identity.\n" % int(id_min*100))
	html.write("\n%s\n" % HEATMAP_SCRIPT)
	html.write("</body></html>")

def writeHist(out, REPEAT_CONTENT, cleanLncs):
	pre = out+"_html/"
	#make histogram
	before_hist = [0]*20

	total = 0
	for x in REPEAT_CONTENT:
		total += 1
		index = int(math.floor(x[1] * 20))
		if index>= 20: index = 19
		before_hist[index] = before_hist[index]+1
	
	after_hist = [0]*20

	after_total = 0
	for x in REPEAT_CONTENT:
		lnc = x[0]
		content = x[1]
		if lnc in cleanLncs:
			after_total += 1
			index = int(math.floor(content*20))
			if index >=20: index = 19
			after_hist[index] = after_hist[index] + 1
	
	repeat = open(pre+out+".repeat_histogram.txt", 'w')
	repeat.write("letter\tfrequency_b\tfrequency_a\n")
	for i in range(len(before_hist)):
		repeat.write("%d-%d%%\t%.2f\t%.2f\n" %(i*5, (i*5)+5, (before_hist[i]*1.0) / total, after_hist[i]*1.0 / after_total))


REPEAT_HEAD = '''
<style>

.bar {
	  fill: steelblue;
}

.bar:hover {
	  fill: brown;
}

.axis {
	  font: 10px sans-serif;
}

.axis path,
.axis line {
	  fill: none;
	    stroke: #000;
		  shape-rendering: crispEdges;
}

.x.axis path {
	  display: none;
}

</style>
'''

REPEAT_SCRIPT = '''
<form>
<label><input type="radio" name="dataset" value="before" checked> Before Filtering</label>
<label><input type="radio" name="dataset" value="after"> After Filtering</label>
</form>

<script src="http://d3js.org/d3.v3.min.js"></script>

<script>

var margin = {top: 20, right: 20, bottom: 30, left: 40},
    width = 960 - margin.left - margin.right,
    height = 500 - margin.top - margin.bottom;

var x = d3.scale.ordinal()
    .rangeRoundBands([0, width], .1);

var y = d3.scale.linear()
    .range([height, 0]);

var xAxis = d3.svg.axis()
    .scale(x)
    .orient("bottom");

var yAxis = d3.svg.axis()
    .scale(y)
    .orient("left")
    .ticks(10, "%");

var svg = d3.select("body").append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom)
  .append("g")
      .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

  d3.tsv("REPLACEME", type, function(error, data) {
    x.domain(data.map(function(d) { return d.letter; }));
  y.domain([0, d3.max(data, function(d) { return d.frequency_b; })]);

    svg.append("g")
      .attr("class", "x axis")
        .attr("transform", "translate(0," + height + ")")
      .call(xAxis);

    svg.append("g")
      .attr("class", "y axis")
        .call(yAxis)
    .append("text")
      .attr("transform", "rotate(-90)")
        .attr("y", 6)
      .attr("dy", ".71em")
        .style("text-anchor", "end")
      .text("Frequency");

    svg.selectAll(".bar")
      .data(data)
      .enter().append("rect")
        .attr("class", "bar")
      .attr("x", function(d) { return x(d.letter); })
        .attr("width", x.rangeBand())
      .attr("y", function(d) { return y(d.frequency_b); })
        .attr("height", function(d) { return height - y(d.frequency_b); });

	d3.selectAll("input").on("change", change);

	function change(){
		var transition = svg.transition().duration(750),
        	delay = function(d, i) { return i * 50; };

		var value = this.value;

		if (value == "before") {
			transition.selectAll(".bar")
				.delay(delay)
				.attr("y", function(d) { return y(d.frequency_b); })
				.attr("height", function(d) { return height - y(d.frequency_b); });
            y.domain([0, d3.max(data, function(d) { return d.frequency_b; })]);
			svg.selectAll("g.y.axis")
				.call(yAxis);
		} else {
			transition.selectAll(".bar")
				.delay(delay)
				.attr("y", function(d) { return y(d.frequency_a); })
				.attr("height", function(d) { return height - y(d.frequency_a); });
		    y.domain([0, d3.max(data, function(d) { return d.frequency_a; })]);
			svg.selectAll("g.y.axis")
				.call(yAxis);
		}
	}
  });

  function type(d) {
    d.frequency_b = +d.frequency_b;
	d.frequency_a = +d.frequency_a;
  return d;
  }

  </script>

'''

def writeRepeat(out, REPEAT_FILTER, REPEAT_CONTENT, repeat_min, cleanLncs):
	global REPEAT_HEAD
	global REPEAT_SCRIPT

	pre = out+"_html/"
	
	REPEAT_SCRIPT = REPEAT_SCRIPT.replace("REPLACEME", out+".repeat_histogram.txt")

	writeHist(out, REPEAT_CONTENT, cleanLncs)
	
	html = open(pre+"repeatContent.html", 'w')
	html.write("<html><head>%s</head><body>\n" % REPEAT_HEAD)
	html.write("<h2>Repeat Content</h2>\n")
	html.write("\n%s\n" % REPEAT_SCRIPT)
	
	html.write("<p>The following transcripts had above %d%% repeat content and were filtered out:" % int(repeat_min*100))
	html.write("<ul>\n")
	for x in REPEAT_FILTER:
		html.write("<li>%s - %.1f%%</li>\n" % (x[0], x[1]*100))
	html.write("</ul>\n")

	html.write("</body></html>")



def writeWebsite(ANNOTS, out, ogLncs, SENSE_FILTER, ANTISENSE_FILTER, SENSE_MAPPED_CODING_FILTER, ANTISENSE_MAPPED_CODING_FILTER, CODING_BLAST_FILTER, SELF_BLAST_FILTER, id_min, REPEAT_FILTER, REPEAT_CONTENT, repeat_min, cleanLncs):

	pre = out+"_html/"
	
	if not os.path.exists(pre):
		cmd = ['mkdir', pre]
		subprocess.check_call(cmd)
	

	index = open(pre+"index.html", 'w')
	index.write("<html>\n<body>\n<h2>filterLncs</h2>\n")
	index.write("<ul>\n")
	if "CODING" in ANNOTS or "MAPPED_CODING" in ANNOTS: 
		index.write("<li><a href=\"overlapCoding.html\">Overlapping Coding Transcripts</a></li>\n")
		writeOverlappingTranscripts(out, ogLncs, SENSE_FILTER, ANTISENSE_FILTER, SENSE_MAPPED_CODING_FILTER, ANTISENSE_MAPPED_CODING_FILTER)
	
	if len(CODING_BLAST_FILTER) > 0:
		writeCodingBlast(out, CODING_BLAST_FILTER)
		index.write("<li><a href=\"blastCoding.html\">Transcripts Blasting to Orthologous Coding</a></li>\n")
	
	if "GENOME_FA" in ANNOTS:
		index.write("<li><a href=\"selfBlast.html\">Duplications</a></li>\n")
		writeDuplications(out, SELF_BLAST_FILTER, id_min)
	
	
	if "REPEAT_MASKER" in ANNOTS: 
		index.write("<li><a href=\"repeatContent.html\">Repeat Content</a></li>\n")
		writeRepeat(out, REPEAT_FILTER, REPEAT_CONTENT, repeat_min, cleanLncs)
	index.write("</ul>\n")
	index.write("</body></html>\n")
	index.close()




	#html.write("</body></html>")
def main():
	parser = argparse.ArgumentParser(description='Filter transcripts for bona fide lncRNAs and searches for orthologs')
	parser.add_argument('bedfile', type=str, help='bed12 file of transcripts')
	parser.add_argument('assembly', type=str, help='assembly')
	parser.add_argument('out_prefix', type=str, help='out_prefix')
	parser.add_argument('--config', type=str, help='path to assembly.config file. default uses config file in same directory as slncky')
	parser.add_argument('--sense_min', type=float, help='min exonic overlap to filter out sense overlapping transcript, default = 0', default = 0)
	parser.add_argument('--antisense_min', type=float, help='min exonic overlap to filter out antisense overlapping transcript, default = 0.3', default=0.3)
	parser.add_argument('--coding_blast_min', type=float, help='min exonic identity to filter out transcript that blasts to orthologous coding gene. default = 0.1', default=0.1)
	parser.add_argument('--selfblast_coverage_min', '-sbc', type=float, help='min coverage when blasting for duplications within transcript. default=0.5', default=0.5)
	parser.add_argument('--selfblast_identity_min', '-sbi', type=float, help='min identity when blasting for duplications within transcript. default=0.5', default=0.5)
	parser.add_argument('--repeat_min', '-r', type=float, help='min repeat content. default = 0.97', default=0.97)
	parser.add_argument('--noncoding_blast_min', type=float, help='min exonic identity to filter out transcript that blasts to orthologous noncoding gene. default=0', default=0.0)
	parser.add_argument('--threads', '-n', type=int, help='number of threads', default=1)
	parser.add_argument('--bedtools_path', type=str, help='path to bedtools')
	parser.add_argument('--liftover_path', type=str, help='path to liftOver')
	parser.add_argument('--lastz_path', type=str, help='path to lastz')
	parser.add_argument('--no_filter', action='store_true', help='flag if you don\'t want lncs to be filtered before searching for ortholog')
	parser.add_argument('--no_orth_search', action='store_true', help='flag if you only want to filter lncs but don\'t want to search for orthologs')
	parser.add_argument('--no_website', action='store_true', help='flag if you don\'t want filter to create website')
	args = parser.parse_args()
	
	global REALPATH
	global ALIGNTRANSCRIPTS
	REALPATH = os.path.realpath(__file__)[0:-9]
	ALIGNTRANSCRIPTS = REALPATH+"alignTranscripts"
	if args.config is None: args.config = REALPATH+"annotations.config"


	if args.bedtools_path is not None:
		global FASTAFROMBED
		global INTERSECTBED
		global BEDTOOLS
		FASTAFROMBED = args.bedtools_path+"/fastaFromBed"
		INTERSECTBED = args.bedtools_path+"/intersectBed"
		BEDTOOLS = args.bedtools_path+"/bedtools"
	if args.liftover_path is not None:
		global LIFTOVER
		LIFTOVER = args.liftover_path+"/liftOver"
	if args.lastz_path is not None:
		global LASTZ
		LASTZ = args.lastz_path+"/lastz"

	checkDependencies()

	#READ IN ANNOTATIONS#
	print "Loading annotations for %s" % args.assembly

	ANNOTS = readAnnots(args.config, args.assembly)

	if "ORTHOLOG" in ANNOTS: 
		#check for liftover file
		if "LIFTOVER" not in ANNOTS:
			print "WARNING: Ortholog was specified for %s but NOT a liftover file.  No ortholog analysis will be carried out!" % args.assembly

		print "Loading ortholog annotations for %s" % ANNOTS["ORTHOLOG"][0]
		ORTH_ANNOTS = readAnnots(args.config, ANNOTS["ORTHOLOG"][0])

	#READ IN LNCS#
	lncs = {}

	bed = open(args.bedfile, 'r')
	for line in bed.readlines():
		splitline = line.split()
		lncs[splitline[3].strip()] = line
	bed.close()
	ogLncs = deepcopy(lncs)
	
	print "\nStarting with %d transcripts" % len(lncs)

	if not args.no_filter:
	
		if os.path.isfile(args.out_prefix+".bed"):
			print "%s.bed already exists! Please choose a different out_prefix." % args.out_prefix
			sys.exit(1)

		print "Removing..."
		#INITIALIZE FILTERS
		SENSE_FILTER = []
		ANTISENSE_FILTER = []
		SENSE_MAPPED_CODING_FILTER = []
		ANTISENSE_MAPPED_CODING_FILTER = []
		CODING_BLAST_FILTER = []
		SELF_BLAST_FILTER = []
		REPEAT_FILTER = []
		REPEAT_CONTENT = []

		#REPEAT_CONTENT
		if "REPEAT_MASKER" in ANNOTS:
			repeatLncs, REPEAT_FILTER, numRemoved, REPEAT_CONTENT = repeatFilter(ogLncs, ANNOTS, args.repeat_min)
			lncs, numRemoved = removeLncs(lncs, REPEAT_FILTER)
			print "        ... %d transcripts with >97%% repeat content" % numRemoved

		#INTERSECT WITH CODING GENES
		if "CODING" in ANNOTS:
			#lncs, SENSE_FILTER, numRemoved = removeSenseOverlap(lncs, ANNOTS)
			
			lncs, SENSE_FILTER, numRemoved = removeOverlap(lncs, ANNOTS["CODING"], ['-s', '-split'], args.sense_min)

			print "        ... %d transcripts that overlap >0%% with coding transcript (sense)" % numRemoved

			#lncs, ANTISENSE_FILTER, numRemoved = removeAntisenseOverlap(lncs, ANNOTS)
			lncs, ANTISENSE_FILTER, numRemoved = removeOverlap(lncs, ANNOTS["CODING"], ['-S', '-split'], args.antisense_min)
			print "        ... %d transcripts that overlap >30%% with coding transcript (antisense)" % numRemoved
		
		if "MAPPED_CODING" in ANNOTS:
			lncs, SENSE_MAPPED_CODING_FILTER, numRemoved = removeOverlap(lncs, ANNOTS["MAPPED_CODING"], ['-s', '-split'], args.sense_min)
			print "        ... %d transcripts that overlap >0%% with mapped coding transcript (sense)" % numRemoved
		
			lncs, ANTISENSE_MAPPED_CODING_FILTER, numRemoved = removeOverlap(lncs, ANNOTS["MAPPED_CODING"], ['-S', '-split'], args.antisense_min)
			print "        ... %d transcripts that overlap >30%% with mapped coding transcript (antisense)" % numRemoved


		#BLAST TO CODING ORTHOLOGS
		if "ORTHOLOG" in ANNOTS and "LIFTOVER" in ANNOTS and "GENOME_FA" in ANNOTS and "GENOME_FA" in ORTH_ANNOTS and "CODING" in ORTH_ANNOTS:
			CODING_BLAST_FILTER = blastToOrtholog(lncs, ANNOTS, ORTH_ANNOTS, "CODING", args.coding_blast_min, args.threads)
			lncs, numRemoved = removeLncs(lncs, CODING_BLAST_FILTER)
			print "\r        ... %d transcripts that blast >10%% to ortholog coding transcript" % numRemoved	

		#SELF-BLAST TO FIND DUPLICATIONS
		if "GENOME_FA" in ANNOTS:
			lncs, SELF_BLAST_FILTER, numRemoved = selfBlast(lncs, ANNOTS, args.selfblast_coverage_min, args.selfblast_identity_min, args.threads)
			#lncs, numRemoved = removeLncs(lncs, SELF_BLAST_FILTER) 
			print "\r        ... %d transcripts that blast >50%% to another transcript in input file" % numRemoved	
		
		#WRITE FILTERED DATA
		
		finalLncs = open(args.out_prefix+".bed", 'w')
		for lnc, line in lncs.iteritems():
			finalLncs.write(line)
		finalLncs.close()

		#WRITE FILTERED TXT
		
		if not args.no_website:
			ucsc = open(args.out_prefix+".OVERLAP.bed", 'w')
			ucscSet = set()
			ucsc.write("track name=overlapping transcripts type=bed visibility=2\n")
		
		filtered = open(args.out_prefix+".filtered.txt", 'w')
		for x in SENSE_FILTER:
			filtered.write("%s\t%.1f%% exonic overlap with coding transcript %s in same orientation\n" % (x[0], x[2]*100.0, x[1]))
			if not args.no_website and x[0] not in ucscSet:
				ucsc.write(ogLncs[x[0]])
				ucscSet.add(x[0])
		for x in ANTISENSE_FILTER:
			filtered.write("%s\t%.1f%% exonic overlap with coding transcript %s in opposite orientation\n" % (x[0], x[2]*100.0, x[1]))
			if not args.no_website and  x[0] not in ucscSet:
				ucsc.write(ogLncs[x[0]])
				ucscSet.add(x[0])
		for x in SENSE_MAPPED_CODING_FILTER:
			filtered.write("%s\t%.1f%% exonic overlap with mapped coding transcript %s in same orientation\n" % (x[0], x[2]*100.0, x[1]))
			if not args.no_website and x[0] not in ucscSet:
				ucsc.write(ogLncs[x[0]])
				ucscSet.add(x[0])
		for x in ANTISENSE_MAPPED_CODING_FILTER:
			filtered.write("%s\t%.1f%% exonic overlap with mapped coding transcript %s in opposite orientation\n" % (x[0], x[2]*100.0, x[1]))
			if not args.no_website and x[0] not in ucscSet:
				ucsc.write(ogLncs[x[0]])
				ucscSet.add(x[0])
		for x in CODING_BLAST_FILTER:
			filtered.write("%s blasts to %s transcript %s with %0.1f%% exonic identity\n" % (x[0], ANNOTS["ORTHOLOG"][0], x[1], float(x[2])*100.0))
		for x in SELF_BLAST_FILTER:
			filtered.write("%s blasts to %s with %0.1f%% identity and %0.1f%% coverage. Appears to be duplication.\n" % (x[0], x[1], float(x[2]), float(x[3])))
		for x in REPEAT_FILTER:
			filtered.write("%s has %.1f%% repeat content.  Appears to be mapping artifact.\n" % (x[0], x[1] * 100))

		#WRITE WEBSITE

		if not args.no_website: writeWebsite(ANNOTS, args.out_prefix, ogLncs, SENSE_FILTER, ANTISENSE_FILTER, SENSE_MAPPED_CODING_FILTER, ANTISENSE_MAPPED_CODING_FILTER, CODING_BLAST_FILTER, SELF_BLAST_FILTER, args.selfblast_identity_min, REPEAT_FILTER, REPEAT_CONTENT, args.repeat_min, lncs)

	if not args.no_orth_search:
		print "Searching for orthologs..."
		#FIND NONCODING ORTHOLOGS!
		if "NONCODING" in ORTH_ANNOTS and "LIFTOVER" in ANNOTS and "GENOME_FA" in ANNOTS and "GENOME_FA" in ORTH_ANNOTS:
			ORTHOLOGS = blastToOrtholog(lncs, ANNOTS, ORTH_ANNOTS, "NONCODING", args.noncoding_blast_min, args.threads)
			tmpLncs, numRemoved = removeLncs(lncs, ORTHOLOGS)
			print "\r        ... %d transcripts with noncoding ortholog!\n" % numRemoved


		#WRITE ORTHOLOGS
		orthologs = open(args.out_prefix+".orthologs.txt", 'w')
		orthologs.write("#lnc\tortholog\texonID\tseqID\n")
		for entry in ORTHOLOGS:
			orthologs.write("%s\t%s\t%.2f\t%.2f\n" % (entry[0], entry[1], entry[2], entry[3]))


if __name__ == "__main__":
	main()
